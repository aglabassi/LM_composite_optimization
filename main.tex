
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{mathtools} 
\usepackage{enumitem} 


\usepackage{xcolor} 
\usepackage{natbib} % Include this package to use the bibliography
\usepackage{hyperref} % Add this line to use the hyperref package

\usepackage{algorithm}
\usepackage{algpseudocode}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{observation}{Observation}

\DeclareMathOperator*{\argmin}{arg\,min}

\definecolor{darkgreen}{RGB}{0,181,90}

% Hyperlink setup
\hypersetup{
    colorlinks=true, % Set to true to enable coloring links
    linkcolor=blue, % Color of internal links
    filecolor=magenta, % Color of file links
    urlcolor=cyan, % Color of external URLs
    citecolor=darkgreen % Color of citations
}

% Define the lemma
\newcommand{\pinv}[1]{#1^{\dagger}}

\begin{document}


\section{Basic Facts About PseudoInverse}
First let's introduce some proprety about the pseudoinverse.

For $M \in \mathbb{K}^{m \times n}$, a pseudoinverse of $M$ is defined as a matrix $M^\dagger \in \mathbb{K}^{n \times m}$ satisfying all of the following four criteria (C1-C4), known as the Mooreâ€“Penrose conditions:

\begin{enumerate}[label=c\arabic*),ref=c\arabic*, start=0]
    \item \label{C0} $(\pinv{M})^* = \pinv{(M^*)} = M^{* \dagger}$
    \item \label{C1} \(M\pinv{M}\) need not be the general identity matrix, but it maps all column vectors of \(M\) to themselves:
    \[M\pinv{M}M = M.\]
    
    \item \label{C2} \(\pinv{M}\) acts like a weak inverse:
    \[\pinv{M}M\pinv{M} = \pinv{M}.\]
    
    \item \label{C3} \(M\pinv{M}\) is Hermitian:
    \[(M\pinv{M})^{*} = M\pinv{M}.\]
    
    \item \label{C4} \(\pinv{M}M\) is also Hermitian:
    \[(\pinv{M}M)^{*} = \pinv{M}M.\]
\end{enumerate}
The pseudo-inverse solves the least-square problems, i.e., 
$$
\text{argmin}_{x \in \mathbb{R}^n} \| Ax - b \|_2^2 =A^\dagger b
$$

% State the lemma
\begin{lemma}
\label{l1}
Let A be a matrix, $A^*$ the conjugate transpose of $A$, $A^\dagger$ the pseudo-inverse of $A$. Then the following holds:

$$
(A^* A)^\dagger = A^\dagger (A^*)^\dagger.
$$
\end{lemma}

% Provide the proof
\begin{proof}
It is sufficient to show that $(A^* A)^\dagger$ satisfie condition 1 of the above. We have :

$$
(A^* A) (A^\dagger A^{* \dagger} ) = A^* \underbrace{A A^{\dagger}}_{=^{\ref{C3}}(AA^{\dagger})^* =^{\ref{C0}} A^{* \dagger} A^* } A^{* \dagger}  = A^* A^{* \dagger} A^* A^{* \dagger}  = ^{\ref{C1}} A^* {A^{* \dagger}}
$$.


Right multiply this by $A^* A$ to verify $\ref{C1}$ is satisfied for $M=A^* A$:

$$
 \underbrace{A^* A^{* \dagger} A^*}_{=^{\ref{C1}} A^*} A = A^* A. 
$$
\end{proof}



\section{Scaled Subgradient Method}

Consider the problem

$$
\min_{X \in \mathbb{R}^{n\times r}} h(c(X))
$$
where $c:\mathbb{R}^{n\times r} \to \mathbb{R}^{n\times n} $ $h:\mathbb{R}^{n\times n} \to \mathbb{R}$
Let's express scaled preconditionner in matrix-vector form. Recall the update direction in matrix-matrix form  for scaled GD:

$$
G \cdot (X^\top X + \lambda I_r)^{-1},
$$
where $G$ a subgradient of $h(c(X))$ with respect to matrix $X$.
Apply $\texttt{vec}$, denote $g=vec(G)$. Using $\texttt{vec}(AB) = (B^{\mathrm{T}} \otimes I_{n})\texttt{vec}(A)$, we have:

\begin{align*}
\text{vec} \big[ G \cdot (X^\top X + \lambda I_r)^{-1} \big] &= ((X^\top X + \lambda I_r)^{-1}\otimes I_{n}) g \\
&= ((X^\top X + \lambda I_r) \otimes I_{n})^{-1} g \\
&\overset{\text{\cite{Langville2004Kronecker}}}{=} ((X^\top X ) \otimes I_n + \lambda I_{nr})^{-1} g
\end{align*}

As $\lambda \to \infty$,  we recover classic GD (i.e. no conditionning at all) as the diagonal dominates, whereas $\lambda = 0$ we recover scaled method. Now recall GN precondionner for nonoverparametrized acts on $g$ as follows\footnote{Since $\nabla c(x)^\top \nabla c(x))^{\dagger} g(x) \overset{\text{L\ref{l1},}g=\nabla c^\top v}{=} \nabla c(x)^\dagger \underbrace{\nabla c(x)^{\top \dagger } \nabla c(x)^\top}_{ = (\nabla c(x) \nabla c(x)^\dagger)^\top \overset{\text{\ref{C3}}}{=} \nabla c(x) \nabla c(x)^\dagger  } v(x)  \overset{\text{\ref{C2}}}{=} \nabla c(x)^\dagger \nabla c(x) \nabla c(x)^\dagger = \nabla c(x)^\dagger v(x).
$} :

$$
(\nabla c(X)^\top \nabla c(X))^{\dagger} g
$$

Thus a naive thing to do to extend GN precondionner for parametrized is 
$$
(\nabla c(X)^\top \nabla c(X) + \lambda I_{nr})^{\dagger} g
$$



\section{GN vs Scaled}

$$
\nabla c(x)^\top \nabla c(x).
$$
This matrix is in general not invertible.


\begin{proposition} (Mateo)
    Let $G \in \mathbb{R}^{nxr}$ be gradient of $f(X)$. Denote $\nabla c(X)^\top \nabla c(X)$ as the linear transfrormation resulting from the jacobian of $c$. Then
    $$
     \nabla c(X)^\top \nabla c(X) G = XG^\top X + GX^\top X. 
    $$ Moreover, for the matrix factorization problem $\min_{X_l X,R} h(c'(X_l,X_r)) $ where $c'(X_l,X_r) = X_l X_r^\top$, we have  
    $$
    \nabla c'(X)^\top \nabla c'(X) G = GX^\top X. 
    $$
    
\end{proposition}
\begin{proof}
    Observe by definition of the jacobian for a direction matrix $V$: $$\nabla C(X) \cdot  V =  \lim_{t\to 0}  \frac{(X+tV)(X+tV)^\top - XX^\top   }{t} = VX^\top + XV^\top$$.
    Let's observe how the transpose acts on some $W$ under the Frebinius inner product
\begin{align*}
    \langle \nabla C^\top V, W \rangle & = \text{tr}(V^\top \nabla C(X) W) \\
    & = \text{tr}( V^\top (WX^\top + XW^\top)) \\
    & = \text{tr}(X^\top V^\top W) + \text{tr}(V^\top X W^\top) \\
    & = \langle V X, W \rangle + \langle V^\top X, W \rangle \\
    & = \langle VX + V^\top X, W \rangle \\
    & \implies \langle \nabla C^\top V -  (VX + V^\top X), W \rangle = 0
\end{align*}

    Since $W$ was arbitrary, the above holds iff $ C^\top V = (V X + V^\top X)$. Now for symmetric $V= \nabla C(X) G = GX^\top + X G^\top$, we conclude.
\end{proof}


\begin{observation}
For the matrix factorization problem, Gauss Newton precondionner is equivalent to the scaled one. When $X^TX$ is invertible, the inverse acts as follow
$$
[] \nabla c'(X)^\top \nabla c'(X) ]^{\dagger} G = \argmin_{D} \| \nabla c'(X)^\top \nabla c'(X) D - G \| = \argmin_{D} \| DX^\top X - G \|  = G(X^TX)^{-1}
$$
\end{observation}

Now for $\nabla c(X)^\top \nabla c(X)$, the following optimization problem is convex thus admit a stationary point 
$$
\min_{D} \| \nabla c'(X)^\top \nabla c'(X) D - G \|^2
$$

It gradient evaluated at $G(X^TX)^{-1}$ is not stationnary point. Therefore, scaled and  GN are not equivalent for matrix recovery problem. Scaled method is a shortcut inspired from matrix factorization problem.







\newpage

\section{GNP Analysis}



Consider the following

$$
\min_{x\in \mathbb{R}^{nr} } f(x):= h(c(x)).

$$

with $c: \mathbb{R}^{nr} \to \mathbb{R}^{nn}$, $h:\mathbb{R}^{nn} \to \mathbb{R}$.




The GNP method relies on approximating what would happen if we would have perform gradient iteration on the image of $c$:
\begin{equation}
\label{eq:1}
    c_{t+1} = c_t - \eta_t v_t   

\end{equation}


with $v_t$ a subgradient of $h(c_t)$. Doing so will remove the dependency on the condition number of $c$, for which the condition number of $X^*$ is related, since we are moving on its inmage. However, such method is unpraticable since $c_{t+1}$ may lie outside the image of $C$, and applying projected gradient may be expansive. Rather, Damet et al. propose to approximate the behaviour of moving on $im(c)$ using preconditionners on $x_t$'s.

Assume $c_t \in im(c)$, and let $x_t$ such that $c_t = c(x_t)$. As in projected GD, we want to find as a next point one that is close to $c_{t+1}$ while still being in the image :

$$
\min_{y} \| c(y) - c_{t+1} \|^2 
$$

To make this optimization problem easier to solve, we can use first-order approximation aroung $x_t$:

$$
\hat{c}(y) = c(x_t) + \nabla c(x_t) \cdot (y-x_t)
$$

Then it is easy to see that  $x_^*$ is a minimizor of $ \min_y \| \hat{c}(y)  - c_{t+1}\|^2$ if and only if
$$
(x^*-x_t) = \eta_t \nabla c(x_t)^\dagger v_t + z
$$

where $z$ is in the nullspace of $\nabla c(x_t)$. The minimizor of $\|x^* - x^t\|$ is obtain when $z=0$. Thus the following update rule :

$$
x_{t+1} = x_{t} - \nabla c(x_t)^\dagger v_t
$$


satisfie
$$
c(x_{t+1}) = \hat{c}(x_{t+1}) + \mathcal{O}(\|\nabla c(x_t)^\dagger v_t \|^2) = c(x_t) - \eta_t \nabla c(x_t) \nabla c(x_t)^\dagger v_t + \mathcal{O}(\|\nabla c(x_t)^\dagger v_t \|^2).
$$
The important thing to observe here is that rather then performing update  \ref{eq:1}, we first transformt the subgradient $v_t$ as $\nabla c(x_t) \nabla c(x_t)^\dagger v_t$, i.e., project $v_t$ into $im(\nabla C)$. Damek et al. work next focussed on showing taking the polyak stepsize on the image, i.e. , $\eta_t = \frac{h(c(x_t))-  h^*}{\|P_{im(\nabla c)} v_t \|^2}$ lead to linear convergence.


\bibliographystyle{plain}
\bibliography{ref}


\end{document}






\end{document}
