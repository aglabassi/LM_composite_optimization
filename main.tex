
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{mathtools} 
\usepackage{enumitem} 


\usepackage{xcolor} 
\usepackage{natbib} % Include this package to use the bibliography
\usepackage{hyperref} % Add this line to use the hyperref package

\usepackage{algorithm}
\usepackage{algpseudocode}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{observation}{Observation}

\DeclareMathOperator*{\argmin}{arg\,min}

\definecolor{darkgreen}{RGB}{0,181,90}

% Hyperlink setup
\hypersetup{
    colorlinks=true, % Set to true to enable coloring links
    linkcolor=blue, % Color of internal links
    filecolor=magenta, % Color of file links
    urlcolor=cyan, % Color of external URLs
    citecolor=darkgreen % Color of citations
}

% Define the lemma
\newcommand{\pinv}[1]{#1^{\dagger}}

\begin{document}


\section{Gauss Newton Preconditionner for Matrix Recovery}
First let's introduce some proprety about the pseudoinverse.

For $M \in \mathbb{K}^{m \times n}$, a pseudoinverse of $M$ is defined as a matrix $M^\dagger \in \mathbb{K}^{n \times m}$ satisfying all of the following four criteria (C1-C4), known as the Mooreâ€“Penrose conditions:

\begin{enumerate}[label=c\arabic*),ref=c\arabic*, start=0]
    \item \label{C0} $(\pinv{M})^* = \pinv{(M^*)} = M^{* \dagger}$
    \item \label{C1} \(M\pinv{M}\) need not be the general identity matrix, but it maps all column vectors of \(M\) to themselves:
    \[M\pinv{M}M = M.\]
    
    \item \label{C2} \(\pinv{M}\) acts like a weak inverse:
    \[\pinv{M}M\pinv{M} = \pinv{M}.\]
    
    \item \label{C3} \(M\pinv{M}\) is Hermitian:
    \[(M\pinv{M})^{*} = M\pinv{M}.\]
    
    \item \label{C4} \(\pinv{M}M\) is also Hermitian:
    \[(\pinv{M}M)^{*} = \pinv{M}M.\]
\end{enumerate}
The pseudo-inverse solves the least-square problems, i.e., 
$$
\text{argmin}_{x \in \mathbb{R}^n} \| Ax - b \|_2^2 =A^\dagger b
$$

% State the lemma
\begin{lemma}
\label{l1}
Let A be a matrix, $A^*$ the conjugate transpose of $A$, $A^\dagger$ the pseudo-inverse of $A$. Then the following holds:

$$
(A^* A)^\dagger = A^\dagger (A^*)^\dagger.
$$
\end{lemma}

% Provide the proof
\begin{proof}
It is sufficient to show that $(A^* A)^\dagger$ satisfie condition 1 of the above. We have :

$$
(A^* A) (A^\dagger A^{* \dagger} ) = A^* \underbrace{A A^{\dagger}}_{=^{\ref{C3}}(AA^{\dagger})^* =^{\ref{C0}} A^{* \dagger} A^* } A^{* \dagger}  = A^* A^{* \dagger} A^* A^{* \dagger}  = ^{\ref{C1}} A^* {A^{* \dagger}}
$$.


Right multiply this by $A^* A$ to verify $\ref{C1}$ is satisfied for $M=A^* A$:

$$
 \underbrace{A^* A^{* \dagger} A^*}_{=^{\ref{C1}} A^*} A = A^* A. 
$$
\end{proof}



\subsection{GNP is not equivalent to Scaled Subgradient}



Consider the following

$$
\min_{x\in \mathbb{R}^{nr} } f(x):= h(c(x)).

$$

with $c: \mathbb{R}^{nr} \to \mathbb{R}^{nn}$, $h:\mathbb{R}^{nn} \to \mathbb{R}$.


Assume $c$ is differentiable with $\nabla c(x)$ the $\mathbb{R}^{nn \times nr}$ jacombian of $c(x)$. Then by the chain rule, for $v(x) \in \partial h ( c(x)) \subseteq \mathbb{R}^{nn}$,  we have $g(x):= \nabla c(x)^{\top} v  \in \partial f(x) \subseteq \mathbb{R}^{nr}$. The Gauss Newton precontionner \cite{davis2022linearly} for $g$ is  $\nabla c ^\top \nabla c$, and when applied to $g$, acts as follows:
\[
(\nabla c(x)^\top \nabla c(x))^{\dagger} g(x) \overset{\text{L\ref{l1},}g=\nabla c^\top v}{=} \nabla c(x)^\dagger \underbrace{\nabla c(x)^{\top \dagger } \nabla c(x)^\top}_{ = (\nabla c(x) \nabla c(x)^\dagger)^\top \overset{\text{\ref{C3}}}{=} \nabla c(x) \nabla c(x)^\dagger  } v(x)  \overset{\text{\ref{C2}}}{=} \nabla c(x)^\dagger \nabla c(x) \nabla c(x)^\dagger = \nabla c(x)^\dagger v(x).
\]


For the matrix recovery problem,  scaled methods preconditionners, which corresponds to the quasi-newton preconditioner with diagonal Hessian's block for the matrix factorization problem \cite{Tong_smooth}, an easier problem then matrix recovery, are the most popular \cite{Tong_smooth,Tong_nonsmooth}. This preconditioner works well in practice for related matrix problems, however, it does not adapt to problem structure since it has been tailored for the factorization problem. A more general preconditioner is the Gauss Newton's one which better approximate the true second-order curvature information required for effective preconditioning. Let's start by verifying Gauss Newton preconditioner for matrix recovery does not reduce to scaled preconditioner, i.e., we want to see if :


$$
\texttt{unvec}\big[(\nabla c^\top \nabla c)^\dagger g  \big]  = \texttt{unvec}(g) \cdot (\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1}.
$$
Apply $\texttt{vec}$ everywhere:

$$
\big[(\nabla c^\top \nabla c)^\dagger g  \big]  = \texttt{vec} \big[ \texttt{unvec}(g) \cdot (\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1} \big ]
$$
Using $\texttt{vec}(AB) = (B^{\mathrm{T}} \otimes I_{n})\texttt{vec}(A)$ for $A=\texttt{unvec}(g), B=(\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1}$, we want to check if:
$$
(\nabla c^\top \nabla c)^\dagger  g = ((\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1}\otimes I_{n}) g
$$



\subsection{Making Gauss Newton robust to overparametrization}
In overparametrization setting, as we get closer to $x^*$, scaled's precondionner $P_{scaled}:=(\texttt{unvec}(x)^\top \texttt{unvec}(x))\otimes I_{n}$ may be almost singular.  \cite{xu2023power} propose to add a term $+\lambda I_{r}$ to make it less singular: $$
(\texttt{unvec}(x)^\top \texttt{unvec}(x) + \lambda I_r)\otimes I_{n} \overset{\text{\cite{Langville2004Kronecker}}}{=} (\texttt{unvec}(x)^\top \texttt{unvec}(x) ) \otimes I_n + \lambda I_{nr}
$$
As $\lambda \to \infty$,  we recover classic GD (i.e. no conditionning at all) as the diagonal dominates, whereas $\lambda = 0$ we recover scaled method. Now let's observe Gauss Newton preconditionner:
$$
\nabla c(x)^\top \nabla c(x).
$$
This matrix is in general not invertible.


\begin{proposition} (Mateo)
    Let $G \in \mathbb{R}^{nxr}$ be gradient of $f(X)$. Denote $\nabla c(X)^\top \nabla c(X)$ as the linear transfrormation resulting from the jacobian of $c$. Then
    $$
     \nabla c(X)^\top \nabla c(X) G = XG^\top X + GX^\top X. 
    $$ Moreover, for the matrix factorization problem $\min_{X_l X,R} h(c'(X_l,X_r)) $ where $c'(X_l,X_r) = X_l X_r^\top$, we have  
    $$
    \nabla c'(X)^\top \nabla c'(X) G = GX^\top X. 
    $$
    
\end{proposition}
\begin{proof}
    Observe by definition of the jacobian for a direction matrix $V$: $$\nabla C(X) \cdot  V =  \lim_{t\to 0}  \frac{(X+tV)(X+tV)^\top - XX^\top   }{t} = VX^\top + XV^\top$$.
    Let's observe how the transpose acts on some $W$ under the Frebinius inner product
\begin{align*}
    \langle \nabla C^\top V, W \rangle & = \text{tr}(V^\top \nabla C(X) W) \\
    & = \text{tr}( V^\top (WX^\top + XW^\top)) \\
    & = \text{tr}(X^\top V^\top W) + \text{tr}(V^\top X W^\top) \\
    & = \langle V X, W \rangle + \langle V^\top X, W \rangle \\
    & = \langle VX + V^\top X, W \rangle \\
    & \implies \langle \nabla C^\top V -  (VX + V^\top X), W \rangle = 0
\end{align*}

    Since $W$ was arbitrary, the above holds iff $ C^\top V = (V X + V^\top X)$. Now for symmetric $V= \nabla C(X) G = GX^\top + X G^\top$, we conclude.
\end{proof}


\begin{observation}
For the matrix factorization problem, Gauss Newton precondionner is equivalent to the scaled one. When $X^TX$ is invertible, the inverse acts as follow
$$
[] \nabla c'(X)^\top \nabla c'(X) ]^{\dagger} G = \argmin_{D} \| \nabla c'(X)^\top \nabla c'(X) D - G \| = \argmin_{D} \| DX^\top X - G \|  = G(X^TX)^{-1}
$$
\end{observation}

Now for $\nabla c(X)^\top \nabla c(X)$, the following optimization problem is convex thus admit a stationary point 
$$
\min_{D} \| \nabla c'(X)^\top \nabla c'(X) D - G \|^2
$$

It gradient evaluated at $G(X^TX)^{-1}$ is not stationnary point. Therefore, scaled and  GN are not equivalent for matrix recovery problem. Scaled method is a shortcut inspired from matrix factorization problem.

\bibliographystyle{plain}
\bibliography{ref}


\end{document}






\end{document}
