\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{mathtools} 
\usepackage{enumitem} 


\usepackage{xcolor} 
\usepackage{natbib} % Include this package to use the bibliography
\usepackage{hyperref} % Add this line to use the hyperref package

\usepackage{algorithm}
\usepackage{algpseudocode}



\definecolor{darkgreen}{RGB}{0,181,90}

% Hyperlink setup
\hypersetup{
    colorlinks=true, % Set to true to enable coloring links
    linkcolor=blue, % Color of internal links
    filecolor=magenta, % Color of file links
    urlcolor=cyan, % Color of external URLs
    citecolor=darkgreen % Color of citations
}

% Define the lemma
\newtheorem{lemma}{Lemma}
\newcommand{\pinv}[1]{#1^{\dagger}}

\begin{document}


\section{GNP for Matrix Recovery}
First let's introduce some proprety about the pseudoinverse.

For $M \in \mathbb{K}^{m \times n}$, a pseudoinverse of $M$ is defined as a matrix $M^\dagger \in \mathbb{K}^{n \times m}$ satisfying all of the following four criteria (C1-C4), known as the Mooreâ€“Penrose conditions:

\begin{enumerate}[label=c\arabic*),ref=c\arabic*, start=0]
    \item \label{C0} $(\pinv{M})^* = \pinv{(M^*)} = M^{* \dagger}$
    \item \label{C1} \(M\pinv{M}\) need not be the general identity matrix, but it maps all column vectors of \(M\) to themselves:
    \[M\pinv{M}M = M.\]
    
    \item \label{C2} \(\pinv{M}\) acts like a weak inverse:
    \[\pinv{M}M\pinv{M} = \pinv{M}.\]
    
    \item \label{C3} \(M\pinv{M}\) is Hermitian:
    \[(M\pinv{M})^{*} = M\pinv{M}.\]
    
    \item \label{C4} \(\pinv{M}M\) is also Hermitian:
    \[(\pinv{M}M)^{*} = \pinv{M}M.\]
\end{enumerate}
The pseudo-inverse solves the least-square problems, i.e., 
$$
\text{argmin}_{x \in \mathbb{R}^n} \| Ax - b \|_2^2 =A^\dagger b
$$

% State the lemma
\begin{lemma}
\label{l1}
Let A be a matrix, $A^*$ the conjugate transpose of $A$, $A^\dagger$ the pseudo-inverse of $A$. Then the following holds:

$$
(A^* A)^\dagger = A^\dagger (A^*)^\dagger.
$$
\end{lemma}

% Provide the proof
\begin{proof}
It is sufficient to show that $(A^* A)^\dagger$ satisfie condition 1 of the above. We have :

$$
(A^* A) (A^\dagger A^{* \dagger} ) = A^* \underbrace{A A^{\dagger}}_{=^{\ref{C3}}(AA^{\dagger})^* =^{\ref{C0}} A^{* \dagger} A^* } A^{* \dagger}  = A^* A^{* \dagger} A^* A^{* \dagger}  = ^{\ref{C1}} A^* {A^{* \dagger}}
$$.


Right multiply this by $A^* A$ to verify $\ref{C1}$ is satisfied for $M=A^* A$:

$$
 \underbrace{A^* A^{* \dagger} A^*}_{=^{\ref{C1}} A^*} A = A^* A. 
$$
\end{proof}



\subsection{GNP is not equivalent to Scaled Subgradient}



Consider the following

$$
\min_{x\in \mathbb{R}^{nr} } f(x):= h(c(x)).

$$

with $c: \mathbb{R}^{nr} \to \mathbb{R}^{nn}$, $h:\mathbb{R}^{nn} \to \mathbb{R}$.


Assume $c$ is differentiable with $\nabla c(x)$ the $\mathbb{R}^{nn \times nr}$ jacombian of $c(x)$. Then by the chain rule, for $v(x) \in \partial h ( c(x)) \subseteq \mathbb{R}^{nn}$,  we have $g(x):= \nabla c(x)^{\top} v  \in \partial f(x) \subseteq \mathbb{R}^{nr}$. The Gauss Newton precontionner \cite{davis2022linearly} for $g$ is  $\nabla c ^\top \nabla c$, and when applied to $g$, acts as follows:
\[
(\nabla c(x)^\top \nabla c(x))^{\dagger} g(x) \overset{\text{L\ref{l1},}g=\nabla c^\top v}{=} \nabla c(x)^\dagger \underbrace{\nabla c(x)^{\top \dagger } \nabla c(x)^\top}_{ = (\nabla c(x) \nabla c(x)^\dagger)^\top \overset{\text{\ref{C3}}}{=} \nabla c(x) \nabla c(x)^\dagger  } v(x)  \overset{\text{\ref{C2}}}{=} \nabla c(x)^\dagger \nabla c(x) \nabla c(x)^\dagger = \nabla c(x)^\dagger v(x).
\]


For the matrix recovery problem,  scaled methods preconditionners, which corresponds to the quasi-newton preconditioner with diagonal Hessian's block for the matrix factorization problem \cite{Tong_smooth}, an easier problem then matrix recovery, are the most popular \cite{Tong_smooth,Tong_nonsmooth}. This preconditioner works well in practice for related matrix problems, however, it does not adapt to problem structure since it has been tailored for the factorization problem. A more general preconditioner is the Gauss Newton's one which better approximate the true second-order curvature information required for effective preconditioning. Let's start by verifying Gauss Newton preconditioner for matrix recovery does not reduce to scaled preconditioner, i.e., we want to see if :


$$
\texttt{unvec}\big[(\nabla c^\top \nabla c)^\dagger g  \big]  = \texttt{unvec}(g) \cdot (\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1}.
$$
Apply $\texttt{vec}$ everywhere:

$$
\big[(\nabla c^\top \nabla c)^\dagger g  \big]  = \texttt{vec} \big[ \texttt{unvec}(g) \cdot (\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1} \big ]
$$
Using $\texttt{vec}(AB) = (B^{\mathrm{T}} \otimes I_{n})\texttt{vec}(A)$ for $A=\texttt{unvec}(g), B=(\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1}$, we want to check if:
$$
(\nabla c^\top \nabla c)^\dagger  g = ((\texttt{unvec}(x)^\top \texttt{unvec}(x))^{-1}\otimes I_{n}) g
$$

Assume the two preconditionners are equal. Taking pseudoinverse of the two (pseudoinverse of pseudoinverse cancels, pseudoinverse of Kronecker product is Kronecker product of pseudo-inverses \cite{Langville2004Kronecker}, pseudoinverse of inverse cancels), we have :

$$
\nabla c^\top \nabla c = ((\texttt{unvec}(x)^\top \texttt{unvec}(x))\otimes I_{n}).
$$

which should not hold for $\nabla c$. Let $A=\nabla c^\top \nabla c, B =((\texttt{unvec}(x)^\top \texttt{unvec}(x))\otimes I_{n})$. For $n=2,r=1, x= [1,1]$, we have
$$
A - B =    \left[ {\begin{array}{cc}
   4 & 2 \\
   2 & 4 \\
  \end{array} } \right].
$$



\subsection{Making GNP robust to overparametrization}
In overparametrization setting, as we get closer to $x^*$, scaled's precondionner $P_{scaled}:=(\texttt{unvec}(x)^\top \texttt{unvec}(x))\otimes I_{n}$ may be almost singular.  \cite{xu2023power} propose to add a term $+\lambda I_{r}$ to force invertability: 
$$P_{scaled(\lambda)}:=(\texttt{unvec}(x)^\top \texttt{unvec}(x) + \lambda I_r)\otimes I_{n} \overset{\text{\cite{Langville2004Kronecker}}}{=} (\texttt{unvec}(x)^\top \texttt{unvec}(x) ) \otimes I_n + \lambda I_{nr}
$$
As $\lambda \to \infty$,  we recover classic GD (i.e. no conditionning at all) as the diagonal dominates, whereas $\lambda = 0$ we recover scaled method which may cause numerical issue when performing inversion. Now let's observe Gauss Newton preconditionner:
$$
\nabla c(x)^\top \nabla c(x).
$$
This matrix is in general not invertible, and therefore this is why the pseudo-inverse is taken. 


\subsection{}


\bibliographystyle{plain}
\bibliography{ref}


\end{document}






\end{document}
